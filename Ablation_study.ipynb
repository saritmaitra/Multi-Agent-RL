{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.optimize import differential_evolution\n",
        "import numpy as np\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, deque\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import time\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "mwkjZ8SqwHdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/retail_price.csv\")"
      ],
      "metadata": {
        "id": "8BOwdntVwP3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_weekly_data(df: pd.DataFrame, min_weeks: int = 4,\n",
        "                        date_col: str = 'month_year') -> Dict[str, Tuple[List[int], List[float], float]]:\n",
        "    \"\"\"weekly product data with demand-weighted reference prices\"\"\"\n",
        "    try:\n",
        "        df['date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "        df = df.dropna(subset=['date'])\n",
        "\n",
        "        df['year_week'] = df['date'].dt.to_period('W').astype(str)\n",
        "\n",
        "        weekly_demand = (\n",
        "            df.groupby(['product_id', 'year_week'], observed=True)['qty']\n",
        "            .sum()\n",
        "            .reset_index()\n",
        "            .sort_values(['product_id', 'year_week']))\n",
        "\n",
        "        product_counts = weekly_demand['product_id'].value_counts()\n",
        "        valid_products = product_counts[product_counts >= min_weeks].index.tolist()\n",
        "\n",
        "        if not valid_products:\n",
        "            raise ValueError(f\"No products with ≥ {min_weeks} weeks of data\")\n",
        "\n",
        "        products_data = {}\n",
        "        for product_id in valid_products:\n",
        "            product_demand = weekly_demand[weekly_demand['product_id'] == product_id]\n",
        "            demand_series = (\n",
        "                product_demand.sort_values('year_week')['qty']\n",
        "                .tolist())\n",
        "\n",
        "            price_df = (\n",
        "                df[df['product_id'] == product_id]\n",
        "                .groupby('year_week', observed=True)[['unit_price', 'qty']]  # Explicitly select columns\n",
        "                .apply(lambda x: np.average(x['unit_price'], weights=x['qty']))\n",
        "                .reset_index(name='weighted_price')\n",
        "                .sort_values('year_week'))\n",
        "            price_series = price_df['weighted_price'].tolist()\n",
        "\n",
        "            product_data = df[df['product_id'] == product_id]\n",
        "            valid_transactions = product_data[product_data['qty'] > 0]\n",
        "\n",
        "            if len(valid_transactions) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                ref_price = np.average(\n",
        "                    valid_transactions['unit_price'],\n",
        "                    weights=valid_transactions['qty'])\n",
        "            except:\n",
        "\n",
        "                ref_price = valid_transactions['unit_price'].median()\n",
        "\n",
        "            min_length = min(len(demand_series), len(price_series))\n",
        "            if min_length < min_weeks:\n",
        "                continue\n",
        "\n",
        "            products_data[product_id] = (\n",
        "                demand_series[:min_length],\n",
        "                price_series[:min_length],\n",
        "                ref_price)\n",
        "\n",
        "        print(f\"Prepared data for {len(products_data)} products (min {min_weeks} weeks)\")\n",
        "        return products_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Data preparation failed: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "products_data = prepare_weekly_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zOOv2INwVnc",
        "outputId": "7a5782af-bc4a-4310-ab60-da644a73965d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared data for 28 products (min 4 weeks)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Initial run**"
      ],
      "metadata": {
        "id": "eDp4ShrA7Jud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_weekly_data(df: pd.DataFrame, min_weeks: int = 4,\n",
        "                        date_col: str = 'month_year') -> Dict[str, Tuple[List[int], List[float], float]]:\n",
        "    try:\n",
        "        df['date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "        df = df.dropna(subset=['date'])\n",
        "\n",
        "        df['year_week'] = df['date'].dt.to_period('W').astype(str)\n",
        "\n",
        "        weekly_demand = (\n",
        "            df.groupby(['product_id', 'year_week'], observed=True)['qty']\n",
        "            .sum()\n",
        "            .reset_index()\n",
        "            .sort_values(['product_id', 'year_week']))\n",
        "\n",
        "        product_counts = weekly_demand['product_id'].value_counts()\n",
        "        valid_products = product_counts[product_counts >= min_weeks].index.tolist()\n",
        "\n",
        "        if not valid_products:\n",
        "            raise ValueError(f\"No products with ≥ {min_weeks} weeks of data\")\n",
        "\n",
        "        products_data = {}\n",
        "        for product_id in valid_products:\n",
        "            product_demand = weekly_demand[weekly_demand['product_id'] == product_id]\n",
        "            demand_series = (\n",
        "                product_demand.sort_values('year_week')['qty']\n",
        "                .tolist())\n",
        "\n",
        "            price_df = (\n",
        "                df[df['product_id'] == product_id]\n",
        "                .groupby('year_week', observed=True)[['unit_price', 'qty']]\n",
        "                .apply(lambda x: np.average(x['unit_price'], weights=x['qty']))\n",
        "                .reset_index(name='weighted_price')\n",
        "                .sort_values('year_week'))\n",
        "            price_series = price_df['weighted_price'].tolist()\n",
        "\n",
        "            product_data = df[df['product_id'] == product_id]\n",
        "            valid_transactions = product_data[product_data['qty'] > 0]\n",
        "\n",
        "            if len(valid_transactions) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                ref_price = np.average(\n",
        "                    valid_transactions['unit_price'],\n",
        "                    weights=valid_transactions['qty'])\n",
        "            except:\n",
        "                ref_price = valid_transactions['unit_price'].median()\n",
        "\n",
        "            min_length = min(len(demand_series), len(price_series))\n",
        "            if min_length < min_weeks:\n",
        "                continue\n",
        "\n",
        "            products_data[product_id] = (\n",
        "                demand_series[:min_length],\n",
        "                price_series[:min_length],\n",
        "                ref_price)\n",
        "\n",
        "        print(f\"Prepared data for {len(products_data)} products (min {min_weeks} weeks)\")\n",
        "        return products_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Data preparation failed: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "class OptimizedSingleProductEnv:\n",
        "    def __init__(self, demand_series: List[int], ref_price: float,\n",
        "                 init_inventory: int = 500,\n",
        "                 lead_time: int = 1,\n",
        "                 elasticity: float = -0.8,\n",
        "                 holding_rate: float = 0.015,\n",
        "                 stockout_penalty: float = 5.0,\n",
        "                 order_cost: float = 0.5,\n",
        "                 safety_stock: int = 10,\n",
        "                 service_weight: float = 100.0):\n",
        "\n",
        "        self.service_weight = service_weight\n",
        "        self.original_demand = [max(1, int(d)) for d in demand_series]\n",
        "        self.ref_price = max(0.1, float(ref_price))\n",
        "        self.weeks = len(self.original_demand)\n",
        "        self.init_inventory = max(0, int(init_inventory))\n",
        "        self.lead_time = min(max(1, int(lead_time)), self.weeks-1)\n",
        "        self.elasticity = max(-5.0, min(0.0, float(elasticity)))\n",
        "        self.holding_rate = max(0.0, float(holding_rate))\n",
        "        self.stockout_penalty = max(0.0, float(stockout_penalty))\n",
        "        self.order_cost = max(0.0, float(order_cost))\n",
        "        self.safety_stock = max(0, int(safety_stock))\n",
        "        self.avg_demand = np.mean(self.original_demand) if self.original_demand else 1\n",
        "        self._calculate_demand_variability()\n",
        "        self.reset()\n",
        "\n",
        "    def _calculate_demand_variability(self):\n",
        "        try:\n",
        "            if self.weeks > self.lead_time:\n",
        "                lt_demands = [sum(self.original_demand[i:i+self.lead_time])\n",
        "                              for i in range(len(self.original_demand)-self.lead_time+1)]\n",
        "                self.std_demand_lead_time = np.std(lt_demands) if len(lt_demands) > 1 else 3.0\n",
        "            else:\n",
        "                self.std_demand_lead_time = np.std(self.original_demand) if len(self.original_demand) > 1 else 3.0\n",
        "        except:\n",
        "            self.std_demand_lead_time = 3.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_week = 0\n",
        "        self.inventory = self.init_inventory\n",
        "        self.total_profit = 0\n",
        "        self.total_demand = 0\n",
        "        self.total_sales = 0\n",
        "        self.weekly_stats = []\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        return {\n",
        "            'inventory': self.inventory,\n",
        "            'current_week': self.current_week,\n",
        "            'demand': self.original_demand[self.current_week] if self.current_week < self.weeks else 0,\n",
        "            'avg_demand': np.mean(self.original_demand[:self.current_week+1]) if self.current_week > 0 else 0}\n",
        "\n",
        "    def step(self, price: float, order_qty: int = 0) -> float:\n",
        "        if self.current_week >= self.weeks:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            min_price = self.ref_price * 0.7\n",
        "            price = max(min_price, price)\n",
        "\n",
        "            base_demand = self.original_demand[self.current_week]\n",
        "            price_factor = (max(0.1, price) / self.ref_price) ** self.elasticity\n",
        "            noise = np.random.normal(0, 0.1 * base_demand)\n",
        "            demand = max(1, int(base_demand * price_factor + noise))\n",
        "\n",
        "            self.inventory += order_qty\n",
        "            available = max(self.inventory - self.safety_stock, 0)\n",
        "            sales_qty = min(available, demand)\n",
        "\n",
        "            revenue = sales_qty * price\n",
        "            holding_cost = self.inventory * self.holding_rate\n",
        "            stockout_cost = max(demand - sales_qty, 0) * self.stockout_penalty\n",
        "            order_cost = order_qty * self.order_cost\n",
        "            profit = revenue - holding_cost - stockout_cost - order_cost\n",
        "            service_level = sales_qty / demand if demand > 0 else 0.0\n",
        "\n",
        "            profit_penalty = np.tanh(profit / 1000.0)\n",
        "            service_bonus = self.service_weight * service_level * (1.0 if profit > 0 else 0.2)\n",
        "            inventory_penalty = -0.01 * (self.inventory / max(1, self.avg_demand))\n",
        "\n",
        "            scaled_reward = profit_penalty + service_bonus + inventory_penalty\n",
        "\n",
        "            prev_inventory = self.inventory\n",
        "            self.inventory -= sales_qty\n",
        "            self.total_profit += profit\n",
        "            self.total_demand += demand\n",
        "            self.total_sales += sales_qty\n",
        "\n",
        "            self.weekly_stats.append({\n",
        "                'week': self.current_week,\n",
        "                'price': price,\n",
        "                'order_qty': order_qty,\n",
        "                'inventory': self.inventory,\n",
        "                'demand': demand,\n",
        "                'sales': sales_qty,\n",
        "                'profit': profit,\n",
        "                'service_level': service_level,\n",
        "                'inventory_change': self.inventory - prev_inventory})\n",
        "\n",
        "            self.current_week += 1\n",
        "            return scaled_reward\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in step: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def service_level(self) -> float:\n",
        "        if self.total_demand > 0:\n",
        "            return self.total_sales / self.total_demand\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "class MultiProductRetailEnv:\n",
        "    def __init__(self, products_data: Dict[str, Tuple[List[int], List[float], float]],\n",
        "                 warehouse_capacity: int = 4412,\n",
        "                 shared_transport_cost: float = 0.2):\n",
        "        self.products = {}\n",
        "        self.warehouse_capacity = warehouse_capacity\n",
        "        self.shared_transport_cost = shared_transport_cost\n",
        "        self.current_week = 0\n",
        "        self.shared_shipment_count = 0\n",
        "        self.shared_costs = 0\n",
        "        self.total_orders_this_week = defaultdict(int)\n",
        "        self.weekly_warehouse_utilization = []\n",
        "        self.metrics = {\n",
        "            'shared_costs_total': 0,\n",
        "            'over_utilization_penalties': 0,\n",
        "            'cumulative_rewards': defaultdict(float)}\n",
        "\n",
        "        for product_id, (demand_series, price_series, ref_price) in products_data.items():\n",
        "            try:\n",
        "                self.products[product_id] = {\n",
        "                    'env': OptimizedSingleProductEnv(demand_series, ref_price),\n",
        "                    'pending_orders': [],\n",
        "                    'inventory': 0,\n",
        "                    'space_required': max(1, int(ref_price/15))}\n",
        "            except Exception as e:\n",
        "                print(f\"Error initializing product {product_id}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not self.products:\n",
        "            raise ValueError(\"No valid products initialized\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_week = 0\n",
        "        self.shared_costs = 0\n",
        "        self.shared_shipment_count = 0\n",
        "        self.total_orders_this_week = defaultdict(int)\n",
        "        self.weekly_warehouse_utilization = []\n",
        "        self.metrics = {\n",
        "            'shared_costs_total': 0,\n",
        "            'over_utilization_penalties': 0,\n",
        "            'cumulative_rewards': defaultdict(float)}\n",
        "\n",
        "        for product_id, data in self.products.items():\n",
        "            data['env'].reset()\n",
        "            data['inventory'] = data['env'].init_inventory\n",
        "            data['pending_orders'] = []\n",
        "        return self.get_global_state()\n",
        "\n",
        "    def get_global_state(self):\n",
        "        return {\n",
        "            'current_week': self.current_week,\n",
        "            'warehouse_utilization': self.get_warehouse_utilization(),\n",
        "            'total_orders': sum(self.total_orders_this_week.values()),\n",
        "            'shared_costs': self.shared_costs}\n",
        "\n",
        "    def get_warehouse_utilization(self):\n",
        "        try:\n",
        "            total_used = sum(data['inventory'] * data['space_required'] for data in self.products.values())\n",
        "            return min(1.0, total_used / self.warehouse_capacity)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def step(self, actions: Dict[str, Tuple[float, int]]):\n",
        "        max_weeks = max(len(data['env'].original_demand) for data in self.products.values())\n",
        "        if self.current_week >= max_weeks:\n",
        "            return self.get_global_state(), {}, True, {}\n",
        "\n",
        "        individual_rewards = {}\n",
        "        self.total_orders_this_week.clear()\n",
        "\n",
        "        for product_id, (price, order_qty) in actions.items():\n",
        "            if product_id not in self.products:\n",
        "                continue\n",
        "            if order_qty is None:\n",
        "                max_order = 0\n",
        "            else:\n",
        "                product_data = self.products[product_id]\n",
        "                max_order = self._get_max_order(product_id, order_qty)\n",
        "            self.total_orders_this_week[product_id] = max_order\n",
        "            if max_order > 0:\n",
        "                arrival_week = self.current_week + product_data['env'].lead_time\n",
        "                product_data['pending_orders'].append((arrival_week, max_order))\n",
        "\n",
        "        for product_id, (price, order_qty) in actions.items():\n",
        "            if product_id not in self.products:\n",
        "                continue\n",
        "            product_data = self.products[product_id]\n",
        "            env = product_data['env']\n",
        "            actual_order_qty = self.total_orders_this_week.get(product_id, 0)\n",
        "            profit = env.step(price, actual_order_qty)\n",
        "            product_data['inventory'] = env.inventory\n",
        "            shared_cost_reduction = self.shared_transport_cost * len([pid for pid in actions if self.total_orders_this_week.get(pid, 0) > 0])\n",
        "            individual_rewards[product_id] = profit + shared_cost_reduction\n",
        "\n",
        "        active_orders = sum(1 for qty in self.total_orders_this_week.values() if qty > 0)\n",
        "        if active_orders > 1:\n",
        "            self.shared_shipment_count += 1\n",
        "            self.shared_costs += self.shared_transport_cost * (active_orders - 1)\n",
        "            self.metrics['shared_costs_total'] += self.shared_transport_cost * (active_orders - 1)\n",
        "\n",
        "        self._process_order_arrivals()\n",
        "        utilization = self.get_warehouse_utilization()\n",
        "        self.weekly_warehouse_utilization.append(utilization)\n",
        "\n",
        "        if utilization > 0.85:\n",
        "            penalty = 50 * (utilization - 0.85)\n",
        "            self.metrics['over_utilization_penalties'] += penalty\n",
        "            penalty_per_product = penalty / len(individual_rewards) if individual_rewards else 0\n",
        "            for product_id in individual_rewards:\n",
        "                individual_rewards[product_id] -= penalty_per_product\n",
        "\n",
        "        self.current_week += 1\n",
        "        done = self.current_week >= max_weeks\n",
        "        return self.get_global_state(), individual_rewards, done, {}\n",
        "\n",
        "    def _get_max_order(self, product_id: str, requested_qty: int) -> int:\n",
        "        if product_id not in self.products:\n",
        "            return 0\n",
        "        product_data = self.products[product_id]\n",
        "        space_per_unit = product_data['space_required']\n",
        "        used_space = 0\n",
        "        for pid, data in self.products.items():\n",
        "            current_inventory_space = data['inventory'] * data['space_required']\n",
        "            pending_space = sum(qty * data['space_required'] for week, qty in data['pending_orders'] if week > self.current_week)\n",
        "            used_space += current_inventory_space + pending_space\n",
        "        available_space = max(0, self.warehouse_capacity - used_space)\n",
        "        max_possible = available_space / space_per_unit\n",
        "        return min(requested_qty, int(max_possible))\n",
        "\n",
        "    def _process_order_arrivals(self):\n",
        "        for product_id, product_data in self.products.items():\n",
        "            arrived_qty = sum(qty for week, qty in product_data['pending_orders'] if week == self.current_week)\n",
        "            product_data['inventory'] += arrived_qty\n",
        "            product_data['pending_orders'] = [\n",
        "                (week, qty) for week, qty in product_data['pending_orders']\n",
        "                if week > self.current_week]\n",
        "\n",
        "    def summarize(self):\n",
        "        summary = {\n",
        "            'total_weeks': self.current_week,\n",
        "            'avg_utilization': np.mean(self.weekly_warehouse_utilization) if self.weekly_warehouse_utilization else 0,\n",
        "            'shared_shipments': self.shared_shipment_count,\n",
        "            'shared_costs_total': self.metrics['shared_costs_total'],\n",
        "            'over_utilization_penalties': self.metrics['over_utilization_penalties'],\n",
        "            'cumulative_rewards': dict(self.metrics['cumulative_rewards']),\n",
        "            'product_service_levels': {pid: round(data['env'].service_level(), 3)\n",
        "                                       for pid, data in self.products.items()}}\n",
        "        return summary\n",
        "\n",
        "class OptimizedProductAgent(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int = 5,\n",
        "                 hidden_size: int = 64,\n",
        "                 price_bins: int = 10,\n",
        "                 order_bins: int = 11,\n",
        "                 lr: float = 0.00005,\n",
        "                 batch_size: int = 256,\n",
        "                 target_update_freq: int = 5,\n",
        "                 service_weight: float = 20.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.price_bins_count = price_bins\n",
        "        self.order_bins_count = order_bins\n",
        "        self.action_size = price_bins * order_bins\n",
        "\n",
        "        self.price_bins = np.linspace(0.8, 1.5, self.price_bins_count)\n",
        "        self.order_bins = np.linspace(0, 50, self.order_bins_count)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, self.action_size))\n",
        "\n",
        "        self.optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=0.01)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='max', factor=0.5, patience=50)\n",
        "        self.memory = []\n",
        "        self.gamma = 0.97\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.tau = 0.01\n",
        "        self.steps = 0\n",
        "        self.best_avg_reward = -float('inf')\n",
        "        self.patience_counter = 0\n",
        "        self.max_patience = 100\n",
        "        self.service_weight = service_weight\n",
        "\n",
        "        self.target_net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, self.action_size))\n",
        "        self._update_target_net()\n",
        "\n",
        "    def _update_target_net(self):\n",
        "        for target_param, param in zip(self.target_net.parameters(), self.net.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def get_action(self, state: Dict) -> Tuple[float, int]:\n",
        "        if random.random() < self.epsilon:\n",
        "            price_idx = random.randint(0, len(self.price_bins)-1)\n",
        "            order_idx = random.randint(0, len(self.order_bins)-1)\n",
        "            return self.price_bins[price_idx], self.order_bins[order_idx]\n",
        "\n",
        "        state_tensor = torch.FloatTensor([\n",
        "            state['inventory'],\n",
        "            state['demand'],\n",
        "            state['avg_demand'],\n",
        "            state['global']['warehouse_utilization'],\n",
        "            state['global']['current_week'] / 52])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self(state_tensor)\n",
        "            q_values_2d = q_values.view(self.price_bins_count, self.order_bins_count)\n",
        "            max_idx = torch.argmax(q_values_2d)\n",
        "            price_idx = max_idx // self.order_bins_count\n",
        "            order_idx = max_idx % self.order_bins_count\n",
        "\n",
        "        return self.price_bins[price_idx], self.order_bins[order_idx]\n",
        "\n",
        "    def act(self, state: List) -> Tuple[float, int]:\n",
        "        state = np.array(state)\n",
        "        state_normalized = (state - state.mean()) / (state.std() + 1e-8)\n",
        "\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.uniform(0.8, 1.2), random.randint(0, 30)  # Tighter exploration\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state_normalized)\n",
        "        with torch.no_grad():\n",
        "            q_values = self(state_tensor.unsqueeze(0))\n",
        "            action_idx = torch.argmax(q_values).item()\n",
        "            price_idx = action_idx // self.order_bins_count\n",
        "            order_idx = action_idx % self.order_bins_count\n",
        "            price_factor = self.price_bins[price_idx]\n",
        "            order_qty = self.order_bins[order_idx]\n",
        "        return price_factor, int(order_qty)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done=False):\n",
        "        if isinstance(state, dict):\n",
        "            price, order = action\n",
        "            price_idx = np.argmin(np.abs(self.price_bins - price))\n",
        "            order_idx = np.argmin(np.abs(self.order_bins - order))\n",
        "            action_idx = price_idx * self.order_bins_count + order_idx\n",
        "            state_tensor = [\n",
        "                state['inventory'],\n",
        "                state['demand'],\n",
        "                state['avg_demand'],\n",
        "                state['global']['warehouse_utilization'],\n",
        "                state['global']['current_week'] / 52]\n",
        "            next_state_tensor = [\n",
        "                next_state['inventory'],\n",
        "                next_state['current_week'],\n",
        "                next_state['demand'],\n",
        "                next_state['avg_demand'],\n",
        "                next_state['global']['warehouse_utilization'],\n",
        "                next_state['global']['current_week'] / 52] if next_state is not None else [0,0,0,0,0,0]\n",
        "        else:\n",
        "            price_factor, order_qty = action\n",
        "            price_idx = np.argmin(np.abs(self.price_bins - price_factor))\n",
        "            order_idx = np.argmin(np.abs(self.order_bins - order_qty))\n",
        "            action_idx = price_idx * self.order_bins_count + order_idx\n",
        "            state_tensor = state\n",
        "            next_state_tensor = next_state if next_state is not None else [0,0,0,0]\n",
        "\n",
        "        state_tensor_t = torch.FloatTensor(state_tensor)\n",
        "        with torch.no_grad():\n",
        "            current_q = self(state_tensor_t.unsqueeze(0))[0, action_idx].item()\n",
        "            next_state_tensor_t = torch.FloatTensor(next_state_tensor)\n",
        "            next_q = self.target_net(next_state_tensor_t.unsqueeze(0)).max().item()\n",
        "            td_error = abs(reward + self.gamma * next_q - current_q)\n",
        "\n",
        "        priority = td_error + 1e-5\n",
        "        self.memory.append((state_tensor, action_idx, reward, next_state_tensor, priority))\n",
        "\n",
        "    def replay(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "        if len(self.memory) < batch_size:\n",
        "            return False\n",
        "\n",
        "        priorities = np.array([mem[4] for mem in self.memory])\n",
        "        probabilities = priorities / priorities.sum()\n",
        "        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)\n",
        "\n",
        "        is_weights = (len(self.memory) * probabilities[indices]) ** -0.4\n",
        "        is_weights = torch.FloatTensor(is_weights / is_weights.max())\n",
        "\n",
        "        batch = [self.memory[i] for i in indices]\n",
        "        states, action_indices, rewards, next_states, _ = zip(*batch)\n",
        "\n",
        "        state_tensors = torch.FloatTensor(states)\n",
        "        next_state_tensors = torch.FloatTensor(next_states)\n",
        "        action_tensors = torch.LongTensor(action_indices).unsqueeze(1)\n",
        "        reward_tensors = torch.FloatTensor(rewards)\n",
        "\n",
        "        current_q = self(state_tensors).gather(1, action_tensors)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_state_tensors).max(1)[0]\n",
        "            target_q = reward_tensors + self.gamma * next_q\n",
        "\n",
        "        loss = (is_weights * nn.SmoothL1Loss(reduction='none')(current_q.squeeze(), target_q)).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=0.5)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        recent_rewards = [mem[2] for mem in self.memory[-100:]] if len(self.memory) > 100 else [0]\n",
        "        avg_reward = np.mean(recent_rewards)\n",
        "        self.scheduler.step(avg_reward)\n",
        "\n",
        "        if len(recent_rewards) >= 50:\n",
        "            if avg_reward > self.best_avg_reward:\n",
        "                self.best_avg_reward = avg_reward\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            if self.patience_counter >= self.max_patience:\n",
        "                return True\n",
        "\n",
        "        self.steps += 1\n",
        "        if self.steps % self.target_update_freq == 0:\n",
        "            self._update_target_net()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        return False\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def validate(self):\n",
        "        if len(self.memory) < 50:\n",
        "            return 0.0\n",
        "\n",
        "        recent_losses = []\n",
        "        for i in range(-50, -1):\n",
        "            state, action, reward, next_state, _ = self.memory[i]\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                current_q = self(state_tensor)[0, action].item()\n",
        "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "                next_q = self.target_net(next_state_tensor).max().item()\n",
        "                target_q = reward + self.gamma * next_q\n",
        "                loss = abs(current_q - target_q)\n",
        "                recent_losses.append(loss)\n",
        "\n",
        "        return np.mean(recent_losses)\n",
        "\n",
        "class OptimizedMultiAgentCoordinator:\n",
        "    def __init__(self, products_data: Dict[str, Tuple[List[int], List[float], float]]):\n",
        "        if not products_data:\n",
        "            raise ValueError(\"No products data provided\")\n",
        "\n",
        "        self.env = MultiProductRetailEnv(products_data)\n",
        "        self.agents = {}\n",
        "        self.rewards_history = defaultdict(list)\n",
        "        self.metrics = {'total_episode_rewards': []}\n",
        "        self.convergence_metrics = []\n",
        "\n",
        "        for product_id, (demand_series, price_series, ref_price) in products_data.items():\n",
        "            self.agents[product_id] = OptimizedProductAgent(\n",
        "                input_size=4,\n",
        "                price_bins=10,\n",
        "                order_bins=11,\n",
        "                lr=0.00005)\n",
        "\n",
        "    def train(self, episodes: int = 1000, eval_every: int = 50, batch_size=32):\n",
        "        max_weeks = max([len(data['env'].original_demand) for data in self.env.products.values()])\n",
        "        converged = False\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            if converged:\n",
        "                print(f\"Training converged at episode {ep}\")\n",
        "                break\n",
        "\n",
        "            self.env.reset()\n",
        "            episode_rewards = defaultdict(float)\n",
        "            done = False\n",
        "\n",
        "            for week in range(max_weeks):\n",
        "                actions = {}\n",
        "                for pid, agent in self.agents.items():\n",
        "                    if self.env.products[pid]['env'].current_week < self.env.products[pid]['env'].weeks:\n",
        "                        agent_state_data = self.env.products[pid]['env']._get_state()\n",
        "                        agent_state = [\n",
        "                            agent_state_data['inventory'],\n",
        "                            agent_state_data['current_week'],\n",
        "                            agent_state_data['demand'],\n",
        "                            agent_state_data['avg_demand']]\n",
        "                        price_factor, order_qty = agent.act(agent_state)\n",
        "                        actual_price = price_factor * self.env.products[pid]['env'].ref_price\n",
        "                        actions[pid] = (actual_price, order_qty)\n",
        "                    else:\n",
        "                        actions[pid] = (None, None)\n",
        "\n",
        "                global_state, individual_rewards, done, _ = self.env.step(actions)\n",
        "\n",
        "                for pid, reward in individual_rewards.items():\n",
        "                    if pid in actions and actions[pid][0] is not None:\n",
        "                        episode_rewards[pid] += reward\n",
        "                        agent_state_data = self.env.products[pid]['env']._get_state()\n",
        "                        current_state = [\n",
        "                            agent_state_data['inventory'],\n",
        "                            agent_state_data['current_week'],\n",
        "                            agent_state_data['demand'],\n",
        "                            agent_state_data['avg_demand']]\n",
        "                        next_agent_state_data = self.env.products[pid]['env']._get_state()\n",
        "                        next_state = [\n",
        "                            next_agent_state_data['inventory'],\n",
        "                            next_agent_state_data['current_week'],\n",
        "                            next_agent_state_data['demand'],\n",
        "                            next_agent_state_data['avg_demand']]\n",
        "                        self.agents[pid].remember(\n",
        "                            current_state,\n",
        "                            (price_factor, order_qty),\n",
        "                            reward,\n",
        "                            next_state)\n",
        "                        self.agents[pid].decay_epsilon()\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            convergence_signals = []\n",
        "            for pid, agent in self.agents.items():\n",
        "                validation_loss = agent.validate()\n",
        "                if validation_loss < 1000:\n",
        "                    should_stop = agent.replay(batch_size)\n",
        "                    convergence_signals.append(should_stop)\n",
        "                else:\n",
        "                    print(f\"Warning: Agent {pid} validation loss too high ({validation_loss:.2f}), skipping replay\")\n",
        "                    convergence_signals.append(False)\n",
        "\n",
        "            if all(convergence_signals):\n",
        "                converged = True\n",
        "\n",
        "            total_episode_reward = sum(episode_rewards.values())\n",
        "            self.metrics['total_episode_rewards'].append(total_episode_reward)\n",
        "\n",
        "            for pid, reward in episode_rewards.items():\n",
        "                self.rewards_history[pid].append(reward)\n",
        "\n",
        "            if ep % 20 == 0:\n",
        "                convergence_metrics = self.calculate_convergence_metrics()\n",
        "                self.convergence_metrics.append(convergence_metrics)\n",
        "                status_symbol = \"✓\" if convergence_metrics['convergence_status'] == \"CONVERGED\" else \"➡\" if convergence_metrics['convergence_status'] == \"STABILIZING\" else \"↗\"\n",
        "                print(f\"Episode {ep+1}/{episodes} {status_symbol} | Reward: {total_episode_reward:.2f} | Q-Drift: {convergence_metrics['q_value_drift']:.4f} | Status: {convergence_metrics['convergence_status']}\")\n",
        "\n",
        "            if (ep+1) % eval_every == 0:\n",
        "                print(f\"Episode {ep+1}/{episodes} | Total Reward: {total_episode_reward:.2f}\")\n",
        "\n",
        "        return self.rewards_history, self.env"
      ],
      "metadata": {
        "id": "OpfNhh9JxeQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5StyG3mv-8N",
        "outputId": "7fbc984f-eae7-49eb-80ca-440e39c198bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ablation Study...\n",
            "Running 5 trials per configuration\n",
            "\n",
            "Testing Reward Function Components...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "\n",
            "Testing Learning Parameters...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "\n",
            "Testing Inventory Components...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "\n",
            "Testing Network Architecture...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "   Trial 1/5 for baseline...\n",
            "   Trial 2/5 for baseline...\n",
            "   Trial 3/5 for baseline...\n",
            "   Trial 4/5 for baseline...\n",
            "   Trial 5/5 for baseline...\n",
            "\n",
            "================================================================================\n",
            "ABLATION STUDY RESULTS\n",
            "================================================================================\n",
            "\n",
            "BASELINE PERFORMANCE: $2,029,102.11\n",
            "\n",
            "REWARD FUNCTION COMPONENTS:\n",
            "   no_service_weight        : +   0.8%  (Service: 0.982, Stockouts: 9.0)\n",
            "   high_service_weight      : +   0.2%  (Service: 0.984, Stockouts: 7.8)\n",
            "   no_inventory_penalty     : +   0.6%  (Service: 0.985, Stockouts: 7.4)\n",
            "   no_stockout_penalty      : +   0.5%  (Service: 0.985, Stockouts: 7.6)\n",
            "   no_profit_scaling        : +   0.3%  (Service: 0.983, Stockouts: 8.6)\n",
            "\n",
            "LEARNING PARAMETERS:\n",
            "   high_lr                  : +   0.4%  (Service: 0.985, Stockouts: 7.4)\n",
            "   very_low_lr              : +   0.4%  (Service: 0.982, Stockouts: 8.6)\n",
            "   small_batch              : +   0.9%  (Service: 0.982, Stockouts: 8.8)\n",
            "   large_batch              :   -0.0%  (Service: 0.982, Stockouts: 8.6)\n",
            "   no_target_updates        : +   1.0%  (Service: 0.984, Stockouts: 8.0)\n",
            "   frequent_updates         : +   0.7%  (Service: 0.981, Stockouts: 9.4)\n",
            "\n",
            "INVENTORY MANAGEMENT:\n",
            "   no_safety_stock          : +   0.8%  (Service: 0.981, Stockouts: 9.0)\n",
            "   high_safety_stock        : +   0.4%  (Service: 0.982, Stockouts: 8.4)\n",
            "   no_warehouse_constraints : +   0.3%  (Service: 0.980, Stockouts: 9.4)\n",
            "   long_lead_time           : +   0.3%  (Service: 0.982, Stockouts: 9.0)\n",
            "\n",
            "NETWORK ARCHITECTURE:\n",
            "   small_network            : +   0.7%  (Service: 0.984, Stockouts: 7.8)\n",
            "   large_network            : +   0.4%  (Service: 0.982, Stockouts: 8.8)\n",
            "   no_normalization         : +   0.9%  (Service: 0.982, Stockouts: 8.8)\n",
            "   different_optimizer      : +   0.6%  (Service: 0.982, Stockouts: 8.4)\n",
            "\n",
            "PERFORMANCE IMPACT RANKING:\n",
            "--------------------------------------------------------------------------------\n",
            "Configuration                    Impact  Stability  Service  Stockouts\n",
            "--------------------------------------------------------------------------------\n",
            "no_target_updates                 +1.0%       0.7%    0.984        8.0\n",
            "no_normalization                  +0.9%       0.9%    0.982        8.8\n",
            "small_batch                       +0.9%       0.5%    0.982        8.8\n",
            "no_service_weight                 +0.8%       0.8%    0.982        9.0\n",
            "no_safety_stock                   +0.8%       0.2%    0.981        9.0\n",
            "small_network                     +0.7%       0.4%    0.984        7.8\n",
            "frequent_updates                  +0.7%       1.0%    0.981        9.4\n",
            "no_inventory_penalty              +0.6%       0.3%    0.985        7.4\n",
            "different_optimizer               +0.6%       0.7%    0.982        8.4\n",
            "no_stockout_penalty               +0.5%       0.7%    0.985        7.6\n",
            "high_lr                           +0.4%       0.3%    0.985        7.4\n",
            "large_network                     +0.4%       0.8%    0.982        8.8\n",
            "high_safety_stock                 +0.4%       0.4%    0.982        8.4\n",
            "very_low_lr                       +0.4%       0.4%    0.982        8.6\n",
            "no_warehouse_constraints          +0.3%       0.3%    0.980        9.4\n",
            "long_lead_time                    +0.3%       0.8%    0.982        9.0\n",
            "no_profit_scaling                 +0.3%       0.6%    0.983        8.6\n",
            "high_service_weight               +0.2%       0.5%    0.984        7.8\n",
            "large_batch                       -0.0%       0.4%    0.982        8.6\n",
            "\n",
            "Detailed results saved to 'ablation_study_results.csv'\n",
            "\n",
            "Ablation study completed successfully!\n"
          ]
        }
      ],
      "source": [
        "class AblationStudy:\n",
        "    def __init__(self, products_data, baseline_config):\n",
        "        self.products_data = products_data\n",
        "        self.baseline_config = baseline_config\n",
        "        self.results = {}\n",
        "\n",
        "    def run_ablation_study(self, num_trials=3, episodes=500):\n",
        "        print(\"Starting Ablation Study...\")\n",
        "        print(f\"Running {num_trials} trials per configuration\")\n",
        "\n",
        "        self._test_reward_components(num_trials, episodes)\n",
        "        self._test_learning_parameters(num_trials, episodes)\n",
        "        self._test_inventory_components(num_trials, episodes)\n",
        "        self._test_network_architecture(num_trials, episodes)\n",
        "        self._generate_ablation_report()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def _test_reward_components(self, num_trials, episodes):\n",
        "        \"\"\"Test reward function variations\"\"\"\n",
        "        print(\"\\nTesting Reward Function Components...\")\n",
        "\n",
        "        configurations = {\n",
        "            'baseline': self.baseline_config,\n",
        "            'no_service_weight': self._modify_config(self.baseline_config, {'service_weight': 0}),\n",
        "            'high_service_weight': self._modify_config(self.baseline_config, {'service_weight': 100}),\n",
        "            'no_inventory_penalty': self._modify_config(self.baseline_config, {'inventory_penalty_weight': 0}),\n",
        "            'no_stockout_penalty': self._modify_config(self.baseline_config, {'stockout_penalty': 0}),\n",
        "            'no_profit_scaling': self._modify_config(self.baseline_config, {'profit_scaling': False}),}\n",
        "\n",
        "        for config_name, config in configurations.items():\n",
        "            self.results[config_name] = self._run_configuration(config, num_trials, episodes)\n",
        "\n",
        "    def _test_learning_parameters(self, num_trials, episodes):\n",
        "        \"\"\"Testing learning parameter variations\"\"\"\n",
        "        print(\"\\nTesting Learning Parameters...\")\n",
        "\n",
        "        configurations = {\n",
        "            'high_lr': self._modify_config(self.baseline_config, {'learning_rate': 0.001}),\n",
        "            'very_low_lr': self._modify_config(self.baseline_config, {'learning_rate': 0.00001}),\n",
        "            'small_batch': self._modify_config(self.baseline_config, {'batch_size': 32}),\n",
        "            'large_batch': self._modify_config(self.baseline_config, {'batch_size': 512}),\n",
        "            'no_target_updates': self._modify_config(self.baseline_config, {'target_update_freq': 1000}),\n",
        "            'frequent_updates': self._modify_config(self.baseline_config, {'target_update_freq': 1})}\n",
        "\n",
        "        for config_name, config in configurations.items():\n",
        "            self.results[config_name] = self._run_configuration(config, num_trials, episodes)\n",
        "\n",
        "    def _test_inventory_components(self, num_trials, episodes):\n",
        "        \"\"\"Testing inventory management variations\"\"\"\n",
        "        print(\"\\nTesting Inventory Components...\")\n",
        "\n",
        "        configurations = {\n",
        "            'no_safety_stock': self._modify_config(self.baseline_config, {'safety_stock': 0}),\n",
        "            'high_safety_stock': self._modify_config(self.baseline_config, {'safety_stock': 30}),\n",
        "            'no_warehouse_constraints': self._modify_config(self.baseline_config, {'warehouse_constraints': False}),\n",
        "            'long_lead_time': self._modify_config(self.baseline_config, {'lead_time': 3}),}\n",
        "\n",
        "        for config_name, config in configurations.items():\n",
        "            self.results[config_name] = self._run_configuration(config, num_trials, episodes)\n",
        "\n",
        "    def _test_network_architecture(self, num_trials, episodes):\n",
        "        \"\"\"Testing network architecture variations\"\"\"\n",
        "        print(\"\\nTesting Network Architecture...\")\n",
        "\n",
        "        configurations = {\n",
        "            'small_network': self._modify_config(self.baseline_config, {'hidden_size': 32, 'num_layers': 1}),\n",
        "            'large_network': self._modify_config(self.baseline_config, {'hidden_size': 256, 'num_layers': 4}),\n",
        "            'no_normalization': self._modify_config(self.baseline_config, {'layer_norm': False}),\n",
        "            'different_optimizer': self._modify_config(self.baseline_config, {'optimizer': 'SGD'}),}\n",
        "\n",
        "        for config_name, config in configurations.items():\n",
        "            self.results[config_name] = self._run_configuration(config, num_trials, episodes)\n",
        "\n",
        "    def _modify_config(self, config, modifications):\n",
        "        new_config = deepcopy(config)\n",
        "        new_config.update(modifications)\n",
        "        return new_config\n",
        "\n",
        "    def _run_configuration(self, config, num_trials, episodes):\n",
        "        trial_results = []\n",
        "\n",
        "        for trial in range(num_trials):\n",
        "            print(f\"   Trial {trial+1}/{num_trials} for {config.get('name', 'config')}...\")\n",
        "\n",
        "            env = MultiProductRetailEnv(\n",
        "                self.products_data,\n",
        "                warehouse_capacity=config.get('warehouse_capacity', 4412),\n",
        "                shared_transport_cost=config.get('shared_transport_cost', 0.2))\n",
        "\n",
        "            agents = {}\n",
        "            for product_id, (demand_series, price_series, ref_price) in self.products_data.items():\n",
        "                agents[product_id] = OptimizedProductAgent(\n",
        "                    input_size=config.get('input_size', 4),\n",
        "                    hidden_size=config.get('hidden_size', 64),\n",
        "                    price_bins=config.get('price_bins', 10),\n",
        "                    order_bins=config.get('order_bins', 11),\n",
        "                    lr=config.get('learning_rate', 0.0001),\n",
        "                    batch_size=config.get('batch_size', 256),\n",
        "                    target_update_freq=config.get('target_update_freq', 5),\n",
        "                    service_weight=config.get('service_weight', 20))\n",
        "\n",
        "            coordinator = AblationCoordinator(env, agents)\n",
        "            rewards_history, final_env = coordinator.train(\n",
        "                episodes=episodes,\n",
        "                eval_every=50,\n",
        "                batch_size=config.get('batch_size', 32))\n",
        "\n",
        "            results = coordinator.evaluate(num_episodes=5)\n",
        "            performance_report = self._generate_performance_report(results, final_env)\n",
        "\n",
        "            trial_results.append({\n",
        "                'config': config,\n",
        "                'performance': performance_report,\n",
        "                'rewards_history': rewards_history,\n",
        "                'convergence_metrics': coordinator.convergence_metrics})\n",
        "\n",
        "        return trial_results\n",
        "\n",
        "    def _generate_performance_report(self, results, env):\n",
        "        inventory_metrics = {}\n",
        "\n",
        "        for pid, stats in results.items():\n",
        "            inventory_levels = [w['inventory'] for w in stats['weekly_stats']]\n",
        "            avg_inventory = np.mean(inventory_levels)\n",
        "            inventory_turnover = stats['total_sales'] / avg_inventory if avg_inventory > 0 else 0\n",
        "            stockout_weeks = sum(1 for w in stats['weekly_stats'] if w.get('service_level', 1) < 0.5)\n",
        "            avg_service = np.mean([w.get('service_level', 0) for w in stats['weekly_stats']])\n",
        "\n",
        "            inventory_metrics[pid] = {\n",
        "                'avg_inventory': avg_inventory,\n",
        "                'inventory_turnover': inventory_turnover,\n",
        "                'stockout_weeks': stockout_weeks,\n",
        "                'max_inventory': max(inventory_levels) if inventory_levels else 0,\n",
        "                'min_inventory': min(inventory_levels) if inventory_levels else 0,\n",
        "                'avg_service': avg_service,\n",
        "                'avg_demand': np.mean([w.get('demand', 0) for w in stats['weekly_stats']])}\n",
        "\n",
        "        total_profit = sum(stats['total_profit'] for stats in results.values())\n",
        "\n",
        "        return {\n",
        "            'total_profit': total_profit,\n",
        "            'avg_service_level': np.mean([metrics['avg_service'] for metrics in inventory_metrics.values()]),\n",
        "            'stockout_weeks': sum(metrics['stockout_weeks'] for metrics in inventory_metrics.values()),\n",
        "            'inventory_turnover': np.mean([metrics['inventory_turnover'] for metrics in inventory_metrics.values()]),\n",
        "            'warehouse_utilization': np.mean(env.weekly_warehouse_utilization) if env.weekly_warehouse_utilization else 0}\n",
        "\n",
        "    def _generate_ablation_report(self):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ABLATION STUDY RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        baseline_perf = np.mean([trial['performance']['total_profit'] for trial in self.results['baseline']])\n",
        "\n",
        "        impact_analysis = {}\n",
        "\n",
        "        for config_name, trials in self.results.items():\n",
        "            if config_name == 'baseline':\n",
        "                continue\n",
        "\n",
        "            avg_profit = np.mean([trial['performance']['total_profit'] for trial in trials])\n",
        "            profit_change = ((avg_profit - baseline_perf) / baseline_perf) * 100\n",
        "\n",
        "            profits = [trial['performance']['total_profit'] for trial in trials]\n",
        "            stability = (np.std(profits) / np.mean(profits)) * 100 if np.mean(profits) > 0 else 100\n",
        "\n",
        "            impact_analysis[config_name] = {\n",
        "                'avg_profit': avg_profit,\n",
        "                'profit_change_pct': profit_change,\n",
        "                'stability': stability,\n",
        "                'service_level': np.mean([trial['performance']['avg_service_level'] for trial in trials]),\n",
        "                'stockouts': np.mean([trial['performance']['stockout_weeks'] for trial in trials])}\n",
        "\n",
        "        print(f\"\\nBASELINE PERFORMANCE: ${baseline_perf:,.2f}\")\n",
        "        print(\"\\nREWARD FUNCTION COMPONENTS:\")\n",
        "        self._print_category_results(impact_analysis, ['no_service_weight', 'high_service_weight',\n",
        "                                                     'no_inventory_penalty', 'no_stockout_penalty',\n",
        "                                                     'no_profit_scaling'])\n",
        "\n",
        "        print(\"\\nLEARNING PARAMETERS:\")\n",
        "        self._print_category_results(impact_analysis, ['high_lr', 'very_low_lr', 'small_batch',\n",
        "                                                     'large_batch', 'no_target_updates', 'frequent_updates'])\n",
        "\n",
        "        print(\"\\nINVENTORY MANAGEMENT:\")\n",
        "        self._print_category_results(impact_analysis, ['no_safety_stock', 'high_safety_stock',\n",
        "                                                     'no_warehouse_constraints', 'long_lead_time'])\n",
        "\n",
        "        print(\"\\nNETWORK ARCHITECTURE:\")\n",
        "        self._print_category_results(impact_analysis, ['small_network', 'large_network',\n",
        "                                                     'no_normalization', 'different_optimizer'])\n",
        "\n",
        "\n",
        "        self._generate_impact_ranking(impact_analysis)\n",
        "        self._save_detailed_results(impact_analysis)\n",
        "\n",
        "    def _print_category_results(self, impact_analysis, config_names):\n",
        "        for config_name in config_names:\n",
        "            if config_name in impact_analysis:\n",
        "                data = impact_analysis[config_name]\n",
        "                change_symbol = \"+\" if data['profit_change_pct'] >= 0 else \"\"\n",
        "                print(f\"   {config_name:25s}: {change_symbol}{data['profit_change_pct']:6.1f}%  \"\n",
        "                      f\"(Service: {data['service_level']:.3f}, Stockouts: {data['stockouts']:.1f})\")\n",
        "\n",
        "    def _generate_impact_ranking(self, impact_analysis):\n",
        "        print(\"\\nPERFORMANCE IMPACT RANKING:\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        sorted_impact = sorted(impact_analysis.items(),\n",
        "                             key=lambda x: abs(x[1]['profit_change_pct']),\n",
        "                             reverse=True)\n",
        "\n",
        "        print(f\"{'Configuration':30s} {'Impact':>8s} {'Stability':>10s} {'Service':>8s} {'Stockouts':>10s}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for config_name, data in sorted_impact:\n",
        "            impact_str = f\"{data['profit_change_pct']:+.1f}%\"\n",
        "            stability_str = f\"{data['stability']:.1f}%\"\n",
        "            service_str = f\"{data['service_level']:.3f}\"\n",
        "            stockouts_str = f\"{data['stockouts']:.1f}\"\n",
        "\n",
        "            print(f\"{config_name:30s} {impact_str:>8s} {stability_str:>10s} {service_str:>8s} {stockouts_str:>10s}\")\n",
        "\n",
        "    def _save_detailed_results(self, impact_analysis):\n",
        "        results_df = pd.DataFrame.from_dict(impact_analysis, orient='index')\n",
        "        results_df.to_csv('ablation_study_results.csv')\n",
        "        print(f\"\\nDetailed results saved to 'ablation_study_results.csv'\")\n",
        "\n",
        "class AblationCoordinator:\n",
        "    def __init__(self, env, agents):\n",
        "        self.env = env\n",
        "        self.agents = agents\n",
        "        self.rewards_history = defaultdict(list)\n",
        "        self.convergence_metrics = []\n",
        "\n",
        "    def train(self, episodes=500, eval_every=50, batch_size=32):\n",
        "        max_weeks = max([len(data['env'].original_demand) for data in self.env.products.values()])\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            self.env.reset()\n",
        "            episode_rewards = defaultdict(float)\n",
        "\n",
        "            for week in range(max_weeks):\n",
        "                actions = {}\n",
        "                for pid, agent in self.agents.items():\n",
        "                    if self.env.products[pid]['env'].current_week < self.env.products[pid]['env'].weeks:\n",
        "                        agent_state_data = self.env.products[pid]['env']._get_state()\n",
        "                        agent_state = [\n",
        "                            agent_state_data['inventory'],\n",
        "                            agent_state_data['current_week'],\n",
        "                            agent_state_data['demand'],\n",
        "                            agent_state_data['avg_demand']]\n",
        "                        price_factor, order_qty = agent.act(agent_state)\n",
        "                        actual_price = price_factor * self.env.products[pid]['env'].ref_price\n",
        "                        actions[pid] = (actual_price, order_qty)\n",
        "                    else:\n",
        "                        actions[pid] = (None, None)\n",
        "\n",
        "                global_state, individual_rewards, done, _ = self.env.step(actions)\n",
        "\n",
        "                for pid, reward in individual_rewards.items():\n",
        "                    episode_rewards[pid] += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            for pid, reward in episode_rewards.items():\n",
        "                self.rewards_history[pid].append(reward)\n",
        "\n",
        "        return self.rewards_history, self.env\n",
        "\n",
        "    def evaluate(self, num_episodes=5):\n",
        "        \"\"\"Evaluation method\"\"\"\n",
        "        results = {}\n",
        "        for pid in self.agents.keys():\n",
        "            results[pid] = {\n",
        "                'total_profit': 0,\n",
        "                'total_demand': 0,\n",
        "                'total_sales': 0,\n",
        "                'weekly_stats': []}\n",
        "\n",
        "        max_weeks = max([len(data['env'].original_demand) for data in self.env.products.values()])\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            self.env.reset()\n",
        "            for week in range(max_weeks):\n",
        "                actions = {}\n",
        "                for pid, agent in self.agents.items():\n",
        "                    if self.env.products[pid]['env'].current_week < self.env.products[pid]['env'].weeks:\n",
        "                        agent_state_data = self.env.products[pid]['env']._get_state()\n",
        "                        agent_state = [\n",
        "                            agent_state_data['inventory'],\n",
        "                            agent_state_data['current_week'],\n",
        "                            agent_state_data['demand'],\n",
        "                            agent_state_data['avg_demand']]\n",
        "                        price_factor, order_qty = agent.act(agent_state)\n",
        "                        actual_price = price_factor * self.env.products[pid]['env'].ref_price\n",
        "                        actions[pid] = (actual_price, order_qty)\n",
        "                    else:\n",
        "                        actions[pid] = (None, None)\n",
        "\n",
        "                global_state, individual_rewards, done, _ = self.env.step(actions)\n",
        "\n",
        "                for pid in self.agents.keys():\n",
        "                    if pid in self.env.products:\n",
        "                        env = self.env.products[pid]['env']\n",
        "                        results[pid]['total_profit'] += env.total_profit\n",
        "                        results[pid]['total_demand'] += env.total_demand\n",
        "                        results[pid]['total_sales'] += env.total_sales\n",
        "                        if env.weekly_stats and len(env.weekly_stats) > week:\n",
        "                            results[pid]['weekly_stats'].append(env.weekly_stats[week])\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "        for pid in results.keys():\n",
        "            results[pid]['total_profit'] /= num_episodes\n",
        "            results[pid]['total_demand'] /= num_episodes\n",
        "            results[pid]['total_sales'] /= num_episodes\n",
        "\n",
        "        return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    baseline_config = {\n",
        "        'name': 'baseline',\n",
        "        'learning_rate': 0.0001,\n",
        "        'batch_size': 256,\n",
        "        'target_update_freq': 5,\n",
        "        'service_weight': 20,\n",
        "        'hidden_size': 64,\n",
        "        'num_layers': 2,\n",
        "        'safety_stock': 10,\n",
        "        'warehouse_capacity': 4412,\n",
        "        'stockout_penalty': 5.0,\n",
        "        'inventory_penalty_weight': 0.01}\n",
        "\n",
        "\n",
        "    ablation_study = AblationStudy(products_data, baseline_config)\n",
        "    results = ablation_study.run_ablation_study(num_trials=5, episodes=793)\n",
        "\n",
        "    print(\"\\nAblation study completed successfully!\")"
      ]
    }
  ]
}